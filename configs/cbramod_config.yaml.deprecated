# CBraMod Configuration
# Based on CBraMod paper (ICLR 2025) Table 5 & 6
# Compatible with train_within_subject.py

model:
  name: CBraMod
  d_model: 200              # Model dimension (fixed for pretrained)
  n_layers: 12              # Number of transformer layers
  n_heads: 8                # Number of attention heads
  dim_feedforward: 800      # FFN dimension
  classifier_type: two_layer  # 'two_layer', 'three_layer', or 'one_layer'
  dropout_rate: 0.1         # Dropout probability
  freeze_backbone: false    # Whether to freeze pretrained backbone

data:
  # CBraMod settings
  sampling_rate: 200        # Hz
  window_length: 5.0        # seconds (offline trial duration)

  # Channel configuration
  # CBraMod supports both 19-channel (10-20 system) and 128-channel (full BioSemi)
  # The ACPE (Asymmetric Conditional Positional Encoding) dynamically generates
  # position encodings via convolution, allowing it to handle arbitrary channel counts.
  #
  # Use command line argument to select:
  #   --cbramod-channels 19   # Standard 10-20 system (default, less memory)
  #   --cbramod-channels 128  # Full BioSemi (more spatial info, more memory)
  #
  # Memory considerations:
  #   19ch:  Attention map 19x19=361,     Classifier input 3,800 dim
  #   128ch: Attention map 128x128=16,384, Classifier input 25,600 dim
  #   For 128ch, consider reducing batch_size to 32-64 if OOM occurs.

  # Patch settings (CBraMod uses 1s patches instead of sliding window)
  patch_duration: 1.0       # seconds per patch
  patch_overlap: 0.0        # No overlap

  # Filtering (CBraMod uses wider bandpass)
  bandpass_low: 0.3         # Hz
  bandpass_high: 75.0       # Hz
  notch_freq: 60.0          # Hz (power line noise)

  # Normalization (CBraMod uses divide by 100)
  normalize: divide
  normalize_by: 100.0

training:
  # Following CBraMod paper Table 6
  epochs: 50                # Fewer epochs than EEGNet (50 vs 300)
  batch_size: 128           # Increased for better GPU utilization (was 64)
  optimizer: AdamW
  learning_rate: 1.0e-4     # Base learning rate (for backbone)
  backbone_lr: 1.0e-4       # Learning rate for backbone
  classifier_lr: 5.0e-4     # Learning rate for classifier (5x backbone)
  weight_decay: 0.05        # AdamW weight decay

  # Early stopping
  early_stopping: true
  patience: 10               # Stop if no improvement for 10 epochs
  min_delta: 0.001

  # Learning rate scheduler
  scheduler: cosine         # CosineAnnealingLR
  T_max: 50                 # Match epochs
  min_lr: 1.0e-6

  # Label smoothing for multi-class
  label_smoothing: 0.1

# Data split for cross-subject experiments (NOT used for within-subject)
subjects:
  train: [S01, S02, S03, S04, S05, S06, S07, S08, S09, S10, S11, S12, S13, S14]
  val: [S15, S16, S17]
  test: [S18, S19, S20, S21]

# NOTE: For within-subject training (paper-aligned), use:
#   uv run python -m src.training.train_within_subject --subject S01 --task binary --model cbramod
# This trains individual models per subject using 80/20 train/val split

# Classification tasks
tasks:
  binary:
    classes: [1, 4]  # Thumb vs Pinky
    n_classes: 2
  ternary:
    classes: [1, 2, 4]  # Thumb vs Index vs Pinky
    n_classes: 3
  quaternary:
    classes: [1, 2, 3, 4]  # All four fingers
    n_classes: 4

# Logging and checkpointing
logging:
  log_dir: results/logs/cbramod
  log_interval: 10  # batches

checkpointing:
  save_dir: checkpoints/cbramod
  save_best: true
  save_last: true

# Pretrained weights
pretrained:
  # Auto-detect from github/CBraMod/pretrained_weights/pretrained_weights.pth
  # Or specify custom path here
  weights_path: null
